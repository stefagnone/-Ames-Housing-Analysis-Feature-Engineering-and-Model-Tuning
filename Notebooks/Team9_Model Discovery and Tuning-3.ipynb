{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e58471f7-a1d0-46ae-8b71-08779c1bd32e",
   "metadata": {},
   "source": [
    "<h2>Function Body</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a636b4dd-b832-439c-b754-772bc41d6abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV # train-test split\n",
    "from sklearn.linear_model import Lars                                    # Least Angle Regression model\n",
    "from sklearn.metrics import r2_score, mean_squared_error, make_scorer    # performance metrics for regression\n",
    "from sklearn.preprocessing import StandardScaler                         # preprocessing utility for scaling features\n",
    "import numpy as np                                                       # numerical Python library for array operations\n",
    "import pandas as pd                                                      # data science essentials\n",
    "import scipy.stats                                                       # statistical functions and distributions from SciPy\n",
    "import warnings                                                          # warnings from code                                      \n",
    "from sklearn.exceptions import FitFailedWarning                          # specific warning for fit failures during model training\n",
    "\n",
    "# suppressing warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# defining the function\n",
    "def lars_model(x_data, y_data, seed=1, tuning=True):\n",
    "    \"\"\"\n",
    " | Description of the Least Angle Regression (Lars) model. \n",
    " | \n",
    " | Model Details:\n",
    " |  \n",
    " |  Lars was designed primarily to address regression problems in high-dimensional data scenarios, where the number of predictors significantly exceeds the number of observations. \n",
    " |  Traditional regression models often struggle with the \"curse of dimensionality,\" but Lars is specifically tailored to manage such conditions efficiently[1][2].\n",
    " | \n",
    " |  Lars can be likened to human decision-making, particularly how an individual would assess multiple choices sequentially, adjusting their preferences as more information is gathered.\n",
    " |  Initially, Lars starts without any predictors, similar to a decision-making process with no biases, and then incrementally adds variables, mirroring the way humans refine their judgments progressively[1][2].\n",
    " | \n",
    " |  The advantages of Lars are evident in its capability to process high-dimensional data and handle multicollinearity with stability and efficiency. \n",
    " |  Its unique approach to weighting predictors provides valuable insights into the importance of each variable, aiding in variable selection and enhancing the interpretability of complex datasets[1][2].\n",
    " |  Nonetheless, Lars also comes with disadvantages. Due to its sequential processing nature, it can be computationally demanding when dealing with an extensive number of predictors. \n",
    " |  While it manages multicollinearity effectively, its performance might degrade due to an overwhelming number of correlated variables. \n",
    " |  Lars may not be suitable for datasets with a substantial number of irrelevant features and operates under the presumption of a linear relationship between predictors and the response, \n",
    " |  which is a limitation in scenarios where this assumption does not apply[1][2].\n",
    " | \n",
    " | Model Calculations:\n",
    " |  Lars calculates by minimizing the residual sum of squares (RSS). It progressively incorporates predictors based on their correlation with the residual. \n",
    " |  This is done strategically by choosing the predictor most correlated with the current residual and adjusting its coefficient in the direction of a least squares fit. \n",
    " |  The process continues by minimizing the angle between the coefficient vector and each predictor's correlation with the residual. \n",
    " |  This is why the method is called \"Least Angle Regression.\" The methodology allows for simultaneous adjustments of all predictors' coefficients, \n",
    " |  maintaining equiangular conditions between the steps, which contributes to the model's stability and interpretability[1][2][3][4].\n",
    " |  \n",
    " |  Lars has an automated feature selection mechanism. It adds predictors to the model one by one, based on their correlation with the residual. \n",
    " |  This stepwise addition of predictors continues until a stopping criterion is met, which could be a predefined number of non-zero coefficients or no further reduction in RSS. \n",
    " |  This method of feature selection allows for an interpretable model that highlights the most significant predictors[1][2].\n",
    " | \n",
    " |  Standardizing or normalizing data is not strictly necessary before using Lars, but doing so can improve the model's interpretability and stability. \n",
    " |  Standardization ensures that each feature contributes equally to the model, preventing variables with larger scales from having a disproportionate influence on the model's behavior,\n",
    " |  which is especially important when the model includes regularization terms that assume features are on the same scale[5].\n",
    " | \n",
    " |  Key hyperparameters: \n",
    " |   •\teps: This determines the precision of the solution, impacting when the coefficient path is considered to have converged, which is crucial for the stability and accuracy of the model output.\n",
    " |   •\tfit_intercept: This decides whether an intercept should be included in the model, which can alter the bias of the predictions.\n",
    " |   •\tnormalize: This indicates whether the input variables should be normalized, affecting model performance, particularly when dealing with variables of different scales[1][2][5].\n",
    " |   •\tmax_iter: This controls the maximum number of iterations and thus affects the convergence of the model solution.\n",
    " | \n",
    " | Model Results:\n",
    " |  Upon completion,Lars outputs an extensive set of results, including coefficients for each predictor in the model. \n",
    " |  These coefficients elucidate the strength and direction of the relationship between predictors and the response variable. \n",
    " |  The model also generates an array of alphas, representing the penalty values at which each predictor is introduced, offering insights into their importance[7]. \n",
    " |  Furthermore, Lars provides R-squared values for both training and testing datasets, illustrating the model's explanatory power.\n",
    " |  Mean squared error (MSE) metrics for the training and testing phases aid in evaluating the model's prediction accuracy and generalization capabilities[8]. \n",
    " |  The difference between training and testing R-squared values, known as the train-test gap, serves as an indicator of potential overfitting or underfitting. \n",
    " |  Additionally, the intercept and the evolutionary path of coefficients, available through the fit_path parameter, enrich the model's interpretability, \n",
    " |  assisting in informed decision-making regarding predictor selection[1][9].\n",
    " | \n",
    " | \n",
    " | References:\n",
    " |  1.\tEfron, B., Hastie, T., Johnstone, I., & Tibshirani, R. (2004). \"Least Angle Regression.\" Annals of Statistics, 32(2), 407-499. \n",
    " | https://www-jstor-org.hult.idm.oclc.org/stable/3448465\n",
    " |  2.\tHastie, T., Tibshirani, R., & Friedman, J. (2009). \"The Elements of Statistical Learning: Data Mining, Inference, and Prediction.\" Springer. \n",
    " | https://link.springer.com/book/10.1007/978-0-387-84858-7\n",
    " |  3.\tTibshirani, R. (1996). \"Regression Shrinkage and Selection via the Lasso.\" Journal of the Royal Statistical Society. Series B (Methodological), 58(1), 267-288. \n",
    " | https://webdoc.agsci.colostate.edu/koontz/arec-econ535/papers/Tibshirani%20(JRSS-B%201996).pdf\n",
    " |  4.\tHoerl, A.E., & Kennard, R.W. (1970). \"Ridge Regression: Biased Estimation for Nonorthogonal Problems.\" Technometrics, 12(1), 55-67. \n",
    " | https://homepages.math.uic.edu/~lreyzin/papers/ridge.pdf\n",
    " |  5.\tFriedman, J., Hastie, T., & Tibshirani, R. (2010). \"Regularization Paths for Generalized Linear Models via Coordinate Descent.\" Journal of Statistical Software, 33(1), 1-22. \n",
    " | https://www.jstatsoft.org/article/view/v033i01\n",
    " |  6.\tZou, H., & Hastie, T. (2005). \"Regularization and Variable Selection via the Elastic Net.\" Journal of the Royal Statistical Society: Series B (Statistical Methodology), 67(2), 301-320. \n",
    " | https://web-p-ebscohost-com.hult.idm.oclc.org/ehost/pdfviewer/pdfviewer?vid=0&sid=47bff9ae-303b-48e9-8647-48d621896f8d%40redis\n",
    " |  7.\tMeinshausen, N., & Bühlmann, P. (2010). \"Stability Selection.\" Journal of the Royal Statistical Society: Series B (Statistical Methodology), 72(4), 417-473.\n",
    " | https://rss.onlinelibrary.wiley.com/doi/full/10.1111/j.1467-9868.2010.00740.x\n",
    " |  8.\tBreiman, L. (1996). \"Heuristics of Instability and Stabilization in Model Selection.\" Annals of Statistics, 24(6), 2350-2383. https://www-jstor-org.hult.idm.oclc.org/stable/2242688\n",
    " |  9.\tTibshirani, R., & Taylor, J. (2012). \"Degrees of Freedom in Lasso Problems.\" The Annals of Statistics, 40(2), 1198-1232.\n",
    " | https://www-jstor-org.hult.idm.oclc.org/stable/41713670\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # train-test split with stratification\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "             x_data, \n",
    "             y_data, \n",
    "             test_size=0.2, \n",
    "             random_state=seed)\n",
    "\n",
    "    # instantiating model object\n",
    "    lars = Lars()\n",
    "\n",
    "    # performing hyperparameter tuning \n",
    "    if tuning:\n",
    "        param_grid = { 'fit_intercept': [True, False],               # whether to calculate the intercept for this model                                                                             \n",
    "                       'normalize': [True, False],                   # whether to normalize the regressors (X) before fitting                                                                          \n",
    "                       'eps': scipy.stats.reciprocal(1e-5, 1e-2)}    # range for epsilon, stopping criteria's tolerance                                                                              \n",
    "\n",
    "        scorer = make_scorer(r2_score)\n",
    "       \n",
    "        # randomizedSearchCV object\n",
    "        search = RandomizedSearchCV(estimator           = lars, \n",
    "                                    param_distributions = param_grid, \n",
    "                                    scoring             =scorer, \n",
    "                                    cv                  =5, \n",
    "                                    n_iter              =100,\n",
    "                                    random_state        =seed, \n",
    "                                    n_jobs              =-1)\n",
    "        # fit the hyperparameter search model using the training data\n",
    "        search.fit(X_train, y_train)\n",
    "       \n",
    "        # printing the optimal parameters and number of models evaluated\n",
    "        best_params = search.best_params_\n",
    "        best_params['eps'] = round(best_params['eps'], 7)\n",
    "        print('Best Parameters: ', best_params)\n",
    "        print(\"Number of Models Evaluated:\", search.n_iter)\n",
    "        lars.set_params(**best_params)\n",
    "\n",
    "        \n",
    "    # training the model\n",
    "    lars.fit(X_train, y_train)\n",
    "\n",
    "    # assesing the model\n",
    "    y_train_pred = lars.predict(X_train)\n",
    "    y_test_pred = lars.predict(X_test)\n",
    "    \n",
    "    # zipping each feature name to its coefficient\n",
    "    model_coefficients = zip(X_train.columns, lars.coef_.round(decimals = 4))\n",
    "    # setting up a placeholder list to store model features\n",
    "    coefficient_lst = [('intercept', lars.intercept_.round(decimals = 4))]\n",
    "    # printing out each feature-coefficient pair one by one\n",
    "    for coefficient in model_coefficients:\n",
    "        coefficient_lst.append(coefficient)\n",
    "    \n",
    "    # dynamically printing results\n",
    "    # every numeric result is rounded (for the end user)\n",
    "    results = f\"\"\"\\\n",
    "    \n",
    "    R-Square (training set): {r2_score(y_train, y_train_pred).round(2)}\n",
    "    R-Square (testing set): {r2_score(y_test, y_test_pred).round(2)}\n",
    "    Train-Test Gap: {(abs(r2_score(y_train, y_train_pred) - r2_score(y_test, y_test_pred))).round(2)}\n",
    "    AIC (training set): {(len(y_train) * np.log(mean_squared_error(y_train, y_train_pred)) + 2 * (len(lars.coef_)+1)).round(0)}\n",
    "    AIC (testing set): {(len(y_test) * np.log(mean_squared_error(y_test, y_test_pred)) + 2 * (len(lars.coef_)+1)).round(0)}\n",
    "    BIC (training set): {(len(y_train) * np.log(mean_squared_error(y_train, y_train_pred)) + np.log(len(y_train)) * (len(lars.coef_)+1)).round(0)}\n",
    "    BIC (testing set): {(len(y_test) * np.log(mean_squared_error(y_test, y_test_pred)) + np.log(len(y_test)) * (len(lars.coef_)+1)).round(0)}\n",
    "    \n",
    "    Coefficents\n",
    "    -----------\n",
    "    {pd.DataFrame(data = coefficient_lst, columns = [\"Feature\", \"Coefficient\"])}\"\"\"\n",
    "\n",
    "    print(results)\n",
    "       \n",
    "        \n",
    "    return \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68e3509-4f57-49e4-94a0-d5145266a843",
   "metadata": {},
   "source": [
    "<h2>Model Testing</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13a5df75-05f1-4a67-94aa-ee4c9f83a710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function lars_model in module __main__:\n",
      "\n",
      "lars_model(x_data, y_data, seed=1, tuning=True)\n",
      "    | Description of the Least Angle Regression (Lars) model. \n",
      "    | \n",
      "    | Model Details:\n",
      "    |  What type of problem(s) was Lars originally designed to solve?\n",
      "    |  The Lars model was designed primarily to address regression problems in high-dimensional data scenarios, where the number of predictors significantly exceeds the number of observations. \n",
      "    |  Traditional regression models often struggle with the \"curse of dimensionality,\" but Lars is specifically tailored to manage such conditions efficiently[1][2].\n",
      "    | \n",
      "    |  The Lars model can be likened to human decision-making, particularly how an individual would assess multiple choices sequentially, adjusting their preferences as more information is gathered.\n",
      "    |  Initially, Lars starts without any predictors, similar to a decision-making process with no biases, and then incrementally adds variables, mirroring the way humans refine their judgments progressively[1][2].\n",
      "    | \n",
      "    |  The advantages of the Lars model are evident in its capability to process high-dimensional data and handle multicollinearity with stability and efficiency. \n",
      "    |  Its unique approach to weighting predictors provides valuable insights into the importance of each variable, aiding in variable selection and enhancing the interpretability of complex datasets[1][2].\n",
      "    |  Nonetheless, the Lars model also comes with disadvantages. Due to its sequential processing nature, it can be computationally demanding when dealing with an extensive number of predictors. \n",
      "    |  While it manages multicollinearity effectively, its performance might degrade due to an overwhelming number of correlated variables. \n",
      "    |  Lars may not be suitable for datasets with a substantial number of irrelevant features and operates under the presumption of a linear relationship between predictors and the response, \n",
      "    |  which is a limitation in scenarios where this assumption does not apply[1][2].\n",
      "    | \n",
      "    | Model Calculations:\n",
      "    |  The Lars model calculates by minimizing the residual sum of squares (RSS). It progressively incorporates predictors based on their correlation with the residual. \n",
      "    |  This is done strategically by choosing the predictor most correlated with the current residual and adjusting its coefficient in the direction of a least squares fit. \n",
      "    |  The process continues by minimizing the angle between the coefficient vector and each predictor's correlation with the residual. \n",
      "    |  This is why the method is called \"Least Angle Regression.\" The methodology allows for simultaneous adjustments of all predictors' coefficients, \n",
      "    |  maintaining equiangular conditions between the steps, which contributes to the model's stability and interpretability[1][2][3][4].\n",
      "    |  \n",
      "    |  Lars has an automated feature selection mechanism. It adds predictors to the model one by one, based on their correlation with the residual. \n",
      "    |  This stepwise addition of predictors continues until a stopping criterion is met, which could be a predefined number of non-zero coefficients or no further reduction in RSS. \n",
      "    |  This method of feature selection allows for an interpretable model that highlights the most significant predictors[1][2].\n",
      "    | \n",
      "    |  Standardizing or normalizing data is not strictly necessary before using the Lars model, but doing so can improve the model's interpretability and stability. \n",
      "    |  Standardization ensures that each feature contributes equally to the model, preventing variables with larger scales from having a disproportionate influence on the model's behavior,\n",
      "    |  which is especially important when the model includes regularization terms that assume features are on the same scale[5].\n",
      "    | \n",
      "    |  Key hyperparameters: \n",
      "    |   •  eps: This determines the precision of the solution, impacting when the coefficient path is considered to have converged, which is crucial for the stability and accuracy of the model output.\n",
      "    |   •  fit_intercept: This decides whether an intercept should be included in the model, which can alter the bias of the predictions.\n",
      "    |   •  normalize: This indicates whether the input variables should be normalized, affecting model performance, particularly when dealing with variables of different scales[1][2][5].\n",
      "    |   •  max_iter: This controls the maximum number of iterations and thus affects the convergence of the model solution.\n",
      "    | \n",
      "    | Model Results:\n",
      "    |  Upon completion, the Lars model outputs an extensive set of results, including coefficients for each predictor in the model. \n",
      "    |  These coefficients elucidate the strength and direction of the relationship between predictors and the response variable. \n",
      "    |  The model also generates an array of alphas, representing the penalty values at which each predictor is introduced, offering insights into their importance[7]. \n",
      "    |  Furthermore, Lars provides R-squared values for both training and testing datasets, illustrating the model's explanatory power.\n",
      "    |  Mean squared error (MSE) metrics for the training and testing phases aid in evaluating the model's prediction accuracy and generalization capabilities[8]. \n",
      "    |  The difference between training and testing R-squared values, known as the train-test gap, serves as an indicator of potential overfitting or underfitting. \n",
      "    |  Additionally, the intercept and the evolutionary path of coefficients, available through the fit_path parameter, enrich the model's interpretability, \n",
      "    |  assisting in informed decision-making regarding predictor selection[1][9].\n",
      "    | \n",
      "    | \n",
      "    | References:\n",
      "    |  1.  Efron, B., Hastie, T., Johnstone, I., & Tibshirani, R. (2004). \"Least Angle Regression.\" Annals of Statistics, 32(2), 407-499. \n",
      "    | https://www-jstor-org.hult.idm.oclc.org/stable/3448465\n",
      "    |  2.  Hastie, T., Tibshirani, R., & Friedman, J. (2009). \"The Elements of Statistical Learning: Data Mining, Inference, and Prediction.\" Springer. \n",
      "    | https://link.springer.com/book/10.1007/978-0-387-84858-7\n",
      "    |  3.  Tibshirani, R. (1996). \"Regression Shrinkage and Selection via the Lasso.\" Journal of the Royal Statistical Society. Series B (Methodological), 58(1), 267-288. \n",
      "    | https://webdoc.agsci.colostate.edu/koontz/arec-econ535/papers/Tibshirani%20(JRSS-B%201996).pdf\n",
      "    |  4.  Hoerl, A.E., & Kennard, R.W. (1970). \"Ridge Regression: Biased Estimation for Nonorthogonal Problems.\" Technometrics, 12(1), 55-67. \n",
      "    | https://homepages.math.uic.edu/~lreyzin/papers/ridge.pdf\n",
      "    |  5.  Friedman, J., Hastie, T., & Tibshirani, R. (2010). \"Regularization Paths for Generalized Linear Models via Coordinate Descent.\" Journal of Statistical Software, 33(1), 1-22. \n",
      "    | https://www.jstatsoft.org/article/view/v033i01\n",
      "    |  6.  Zou, H., & Hastie, T. (2005). \"Regularization and Variable Selection via the Elastic Net.\" Journal of the Royal Statistical Society: Series B (Statistical Methodology), 67(2), 301-320. \n",
      "    | https://web-p-ebscohost-com.hult.idm.oclc.org/ehost/pdfviewer/pdfviewer?vid=0&sid=47bff9ae-303b-48e9-8647-48d621896f8d%40redis\n",
      "    |  7.  Meinshausen, N., & Bühlmann, P. (2010). \"Stability Selection.\" Journal of the Royal Statistical Society: Series B (Statistical Methodology), 72(4), 417-473.\n",
      "    | https://rss.onlinelibrary.wiley.com/doi/full/10.1111/j.1467-9868.2010.00740.x\n",
      "    |  8.  Breiman, L. (1996). \"Heuristics of Instability and Stabilization in Model Selection.\" Annals of Statistics, 24(6), 2350-2383. https://www-jstor-org.hult.idm.oclc.org/stable/2242688\n",
      "    |  9.  Tibshirani, R., & Taylor, J. (2012). \"Degrees of Freedom in Lasso Problems.\" The Annals of Statistics, 40(2), 1198-1232.\n",
      "    | https://www-jstor-org.hult.idm.oclc.org/stable/41713670\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(lars_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "154924cb-c41b-404b-9c48-39e8fe98ed7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'eps': 0.0001783, 'fit_intercept': True, 'normalize': True}\n",
      "Number of Models Evaluated: 100\n",
      "    \n",
      "    R-Square (training set): 0.69\n",
      "    R-Square (testing set): 0.76\n",
      "    Train-Test Gap: 0.07\n",
      "    AIC (training set): 50234.0\n",
      "    AIC (testing set): 12400.0\n",
      "    BIC (training set): 50286.0\n",
      "    BIC (testing set): 12440.0\n",
      "    \n",
      "    Coefficents\n",
      "    -----------\n",
      "             Feature  Coefficient\n",
      "0      intercept  -12393.1613\n",
      "1       Lot_Area       0.0930\n",
      "2   Mas_Vnr_Area      62.4633\n",
      "3  Total_Bsmt_SF      48.5242\n",
      "4   First_Flr_SF     100.1824\n",
      "5  Second_Flr_SF     105.6911\n",
      "6    Gr_Liv_Area     -44.9923\n",
      "7    Garage_Area      93.4549\n",
      "8     Porch_Area      35.8498\n"
     ]
    }
   ],
   "source": [
    "# Y variables is Sale_Price\n",
    "\n",
    "# specifying the path and file name\n",
    "file = 'housing_feature_rich.xlsx'\n",
    "\n",
    "# reading the file into Python\n",
    "housing = pd.read_excel(io     = file,\n",
    "                        header = 0   )\n",
    "\n",
    "# remove the 'property_id' column\n",
    "housing.drop(labels  = ['property_id'], axis    = 1, inplace = True)\n",
    "\n",
    "# setting columns\n",
    "x_original = list(housing.loc[ : , 'Lot_Area' : 'Porch_Area' ])\n",
    "original_y = 'Sale_Price'\n",
    "\n",
    "# preparing x-data & y-data\n",
    "x_data = housing[ x_original ]\n",
    "y_data = housing[ original_y ]\n",
    "\n",
    "lars_model(x_data, y_data, seed=1, tuning=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57d87aac-1d56-419d-a06e-bac56a85bd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'eps': 2.76e-05, 'fit_intercept': True, 'normalize': False}\n",
      "Number of Models Evaluated: 100\n",
      "    \n",
      "    R-Square (training set): 0.67\n",
      "    R-Square (testing set): 0.72\n",
      "    Train-Test Gap: 0.05\n",
      "    AIC (training set): -6822.0\n",
      "    AIC (testing set): -1790.0\n",
      "    BIC (training set): -6770.0\n",
      "    BIC (testing set): -1751.0\n",
      "    \n",
      "    Coefficents\n",
      "    -----------\n",
      "             Feature  Coefficient\n",
      "0      intercept      11.0219\n",
      "1       Lot_Area      -0.0000\n",
      "2   Mas_Vnr_Area       0.0001\n",
      "3  Total_Bsmt_SF       0.0003\n",
      "4   First_Flr_SF       0.0005\n",
      "5  Second_Flr_SF       0.0006\n",
      "6    Gr_Liv_Area      -0.0003\n",
      "7    Garage_Area       0.0006\n",
      "8     Porch_Area       0.0002\n"
     ]
    }
   ],
   "source": [
    "# Y variables is log_Sale_Price\n",
    "\n",
    "# specifying the path and file name\n",
    "file = 'housing_feature_rich.xlsx'\n",
    "\n",
    "# reading the file into Python\n",
    "housing = pd.read_excel(io     = file,header = 0   )\n",
    "\n",
    "# remove the 'property_id' column\n",
    "housing.drop(labels  = ['property_id'], axis    = 1, inplace = True)\n",
    "\n",
    "# setting columns\n",
    "x_original = list(housing.loc[ : , 'Lot_Area' : 'Porch_Area' ])\n",
    "original_y = 'log_Sale_Price'\n",
    "\n",
    "# preparing x-data & y-data\n",
    "x_data = housing[ x_original ]\n",
    "y_data = housing[ original_y ]\n",
    "\n",
    "lars_model(x_data, y_data, seed=1, tuning=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
